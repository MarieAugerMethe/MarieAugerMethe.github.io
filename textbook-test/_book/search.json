[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Population, Parameters and Variables\nYou probably have heard of the term population. In statistics, the term population does not refer to people specifically. Instead, it is just an abstract set with all the elements we want to study. These elements can be people, animals, objects, or anything really.\nThe population defines the scope of your study – who or what you want to study. However, what do you want to know? For example, you might be interested in a certain penguin species (so all penguins of said species form your population). But you might want to study the penguins’ lifespan, height, weight, or anything. To answer your research question, you need to collect data.\nSelecting the right variable is crucial and can be challenging sometimes. For example, if you want to study penguins’ size, should you measure their height or weight? What would you measure if you wanted to study people’s talent for music? We will not concern ourselves with these issues here, but I want to emphasize that this issue is very relevant in statistical applications.\nThe variable of interest is an attribute of the elements in the population, meaning that every element can potentially have a unique value for a given variable. If we could measure the variable of interest over all elements in the population, we would obtain the Population Distribution.\nWith the actual population data in hand, we have all the information we need about the population. We can calculate any quantity (e.g., the mean, standard deviation, quantiles) as well as any probability we may want to know (e.g., “What proportion of students got A+ in STAT 550?”), and any quantiles (e.g., 75% of the students earned a grade ).\nFinally, we are frequently interested in some quantities that summarize the population distribution. Common examples are the mean, standard deviation, median, quantiles, min, max, among others. This leads us to our next definition.\nAll of our learning about the population will be through a random sample of that same population, which we shall discuss next.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#population-parameters-and-variables",
    "href": "intro.html#population-parameters-and-variables",
    "title": "1  Introduction",
    "section": "",
    "text": "Definition 1.1 (Population) The population is the set of all elements that we want to study.\n\n\n\nDefinition 1.2 (Variable) A variable is an attribute of the elements in the population.\n\n\n\n\nDefinition 1.3 (Population Distribution) The population distribution is the distribution of the variable of interest over the entire population.\n\n\nExample 1.1 Suppose we are interested in the final grades of all 40 students who took STAT 550 last year. After reviewing the records, the professor gave us all 40 grades.\n\n\nTable 1: The grades of all students who took STAT 550 last year.\n\n\n\nStudent\n\n\nFinal Grade\n\n\nStudent\n\n\nFinal Grade\n\n\nStudent\n\n\nFinal Grade\n\n\nStudent\n\n\nFinal Grade\n\n\n\n\n\nStudent #1\n\n\n70\n\n\nStudent #11\n\n\n68\n\n\nStudent #21\n\n\n56\n\n\nStudent #31\n\n\n64\n\n\n\n\nStudent #2\n\n\n73\n\n\nStudent #12\n\n\n74\n\n\nStudent #22\n\n\n58\n\n\nStudent #32\n\n\n66\n\n\n\n\nStudent #3\n\n\n54\n\n\nStudent #13\n\n\n67\n\n\nStudent #23\n\n\n81\n\n\nStudent #33\n\n\n78\n\n\n\n\nStudent #4\n\n\n68\n\n\nStudent #14\n\n\n80\n\n\nStudent #24\n\n\n77\n\n\nStudent #34\n\n\n71\n\n\n\n\nStudent #5\n\n\n60\n\n\nStudent #15\n\n\n68\n\n\nStudent #25\n\n\n79\n\n\nStudent #35\n\n\n75\n\n\n\n\nStudent #6\n\n\n64\n\n\nStudent #16\n\n\n48\n\n\nStudent #26\n\n\n63\n\n\nStudent #36\n\n\n49\n\n\n\n\nStudent #7\n\n\n57\n\n\nStudent #17\n\n\n66\n\n\nStudent #27\n\n\n63\n\n\nStudent #37\n\n\n61\n\n\n\n\nStudent #8\n\n\n69\n\n\nStudent #18\n\n\n67\n\n\nStudent #28\n\n\n51\n\n\nStudent #38\n\n\n69\n\n\n\n\nStudent #9\n\n\n80\n\n\nStudent #19\n\n\n72\n\n\nStudent #29\n\n\n56\n\n\nStudent #39\n\n\n56\n\n\n\n\nStudent #10\n\n\n74\n\n\nStudent #20\n\n\n52\n\n\nStudent #30\n\n\n67\n\n\nStudent #40\n\n\n63\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Histogram of students’ grades in STAT 550 last year.\n\n\n\n\n\nTable 1 contains all the values present in our population, or the “distribution” of values in the population. We can see the exact grades of all the students. A helpful way to visualize the values in Table 1 is to use a histogram, as shown in Figure 1.1. You can see in Figure 1.1 that we have one peak between 65 and 70, indicating that most students earned a grade in this range. But we can also see, for example, that one students only got a grade above 80.\nAnother plot that can be useful is called the Cumulative Distribution as shown in Figure 1.2. This plot shows us the proportion of the data below specific values. For example, 25% of the grades were below 60, and 50% were below 67.\n\n\n\n\n\n\n\n\nFigure 1.2: Cumulative Distribution Function of students’ grades in STAT 550 last year.\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Parameters) A parameter is a numerical summary that describes a significant feature of a population’s distribution.\n\n\n\n\n\n\n\nWarning: Parameter vs Variable\n\n\n\nA common mistake many students make is to mix up a variable of interest with a parameter of interest. For example, one might be interested in the average dolphin weight, which is a parameter; the average weight is a quantity of the population, not of a single dolphin. However, the dolphin weight is what is being measured, i.e., the variable of interest.\n\n\n\n\n\n\n\n\nNote\n\n\n\nParameters are constants, but usually unknown.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#random-sample-and-statistics",
    "href": "intro.html#random-sample-and-statistics",
    "title": "1  Introduction",
    "section": "1.2 Random Sample and Statistics",
    "text": "1.2 Random Sample and Statistics\nIn practice, the population distribution is unknown, as measuring every individual in a population is impractical. In these cases, we draw a sample from the population to learn about the population distribution.\nThere are many ways one can draw a sample from the population of interest (e.g., simple random sampling, stratified sampling, clustering, multi-stage sampling). However, the techniques studied in this course strongly rely on the concept of independent sample.\n\nDefinition 1.5 (Independent Sample) The selection of one element does not influence the selection of another.\n\nThe problem is that most of the sampling techniques used in practice do not yield an independent sample. Fortunately, the methods we will explore are still widely applicable, even in these cases. But it is worth it for us to comment on a widely used sampling approach, namely Simple Random Sample (SRS), which we will use during the entire course.\n\nSimple Random Sample\nSimple random sampling is one of the most intuitive, yet very effective and reliable method to obtain a sample. The main principle governing a simple random sample is of equal probability, where each member of the population is equally likely to be chosen, ensuring fairness in representation.\nThe Simple Random Sampling can be conducted in two ways: (1) with replacement; or, more commonly, (2) without replacement.\n\nSRS with replacement: we select one element at a time. Once an element is selected, put the element back in the population and select the following element. This approach allows repeated elements in our sample.\nSRS without replacement: we select one element at a time. Once an element is selected, it is removed from the population and we select the following element. This approach does not allow repeated elements in our sample.\n\n\nExample 1.2 Suppose we have a group of 4 people {Varada, Mike, John, Hayley}. We want to take an SRS of size 2. The possible samples are:\n\nwithout replacement: {Varada, Mike}, {Varada, John}, {Mike, John}, {Varada, Hayley}, {Mike, Hayley}, {John, Hayley}\nwith replacement: {Varada, Mike}, {Varada, John}, {Mike, John}, {Varada, Hayley}, {Mike, Hayley}, {John, Hayley}, {Varada, Varada}, {Mike, Mike}, {John, John}, {Hayley, Hayley}\n\n\nYou might be asking yourself, why would we ever want to use SRS with replacement? If you first collected the information from Varada in Example 1.2, why would you allow the possibility of selecting Varada again? If we select the same element twice, we select repeated information and learn nothing new. So, in general, SRS without replacement is more informative, meaning that our parameter estimates will be more precise. But, unfortunately, there is a catch! SRS without replacement is not independent. The lack of independence violates our Definition 1.5. However, as the population size increases compared to the sample size, SRS without replacement behaves very similarly to an independent sample (i.e., the joint distribution of the sample is very similar to the product of the marginal distributions). Remember the rule of thumb: n must be less than 10\\% the population size? This is the reason. But remember it is just a rule of thumb. You will explore this in more detail in ?exr-srs.\nOn the other hand, when we use SRS with replacement, we enforce the independence of the sample, no matter the population size. In addition, as the population size increases, the SRS with replacement becomes as informative as without replacement. Nonetheless, in practice we rarely use SRS with replacement.\nOne last comment about sampling. We will study what we call asymptotic properties of estimators. In other words, how the estimator behaves as the sample size becomes larger and larger (i.e. when the sample size goes to infinity). But what would that mean in the case of a finite population? If we keep increasing our sample size, we will eventually sample the entire population. In such a case, we would have a census, and no statistic is necessary because we would know the true value of the parameters. SRS with replacement allows us to take a sample larger than the population so that those results are still correct. But again, if you can afford (timewise and moneywise) to get a sample larger than the population, just do a census and be done with it. That being said, the asymptotic properties of estimators give us a lot of information about estimators’ behaviour, which is valuable to know.\n\n\nStatistics\nIn many situations, we want to estimate some parameters of the population distribution. We then draw a sample, calculate summary quantities of the values we obtained in our sample, and use them to estimate our parameters. As it turns out, these functions of the sample are called statistics.\n\nDefinition 1.6 (Statistic) Let X_1, \\ldots, X_n be a random sample of a population f_X. Any function T\\left(X_1,\\ldots, X_n\\right) that does not depend on unknown parameters is called a statistic.\n\n\nExample 1.3 Let X_1, \\ldots, X_n be a random sample of a population f_X. The following are examples of commonly used statistics:\n\nSum: T(X_1, \\ldots, X_n) = \\sum_{i=1}^{n}X_i\nSample Mean: T(X_1, \\ldots, X_n) = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_i\nSample Variance: T(X_1, \\ldots, X_n) = S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_i-\\bar{X}\\right)^2\nSample Max: T(X_1, \\ldots, X_n) = \\max\\left\\{X_1, \\ldots, X_n\\right\\}\nSample Min: T(X_1, \\ldots, X_n) = \\min\\left\\{X_1, \\ldots, X_n\\right\\}\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough frequently statistics are summary quantities, they don’t need to be. The identity function, T(X_1, \\ldots, X_n) = \\left(X_1, \\ldots, X_n\\right), for example, is also an statistic but is not a summary quantity.\n\n\nAs a function of random variables, statistics are also random variables. Therefore, a statistic has a probability distribution, which is called sampling distribution.\n\nDefinition 1.7 (Sampling Distribution) The probability distribution of a statistic T is called sampling distribution of T.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#a-practical-note-on-statistical-inference",
    "href": "intro.html#a-practical-note-on-statistical-inference",
    "title": "1  Introduction",
    "section": "1.3 A practical note on statistical inference",
    "text": "1.3 A practical note on statistical inference\nIn this section, we will try to address the gaps between what you have learned in your Introductory Statistical course and the assumptions generally made in this course, such as ?def-random-sample, which is the base of the theory developed in this course. The fundamental difference is how we view the population: finite vs infinite.\nWhen we think of population, we usually think of a group of real people/objects/animals that we want to study. For example, all the voters in a country, all the penguins of a certain species, all students in a university, etc. A common characteristic of these populations is that they are all finite. So then, to calculate the probability of an event in this population, we could just count the proportion of individuals in the population that satisfy the event in question.\nThe idea of finite population affects our discussion mainly in two ways: (1) sampling approaches; and (2) population distribution. We shall now discuss these two points.\n\nSampling finite vs infinite population\nYou probably learned in an introductory statistics course that there are multiple ways that one can draw a sample from the population. For example, one can use Simple Random Sampling, Systematic Sampling, Stratified Sampling, Cluster Sampling, and Multi-Stage Cluster Sampling. Each of these approaches results in samples with different properties.\nWe will not get into the details of these sampling approaches here. In fact, let us restrict our focus to Simple Random Sampling (SRS). SRS can be conducted in two ways: (1) with replacement; or, more commonly, (2) without replacement.\n\nSRS without replacement: we select n different individuals from the population, and each individual has the same probability of being selected.\nSRS with replacement: we select one element at a time. Once an element is selected, put the element back in the population and select the following element. This approach allows repeated elements.\n\n\nExample 1.4 Suppose we have a group of 4 people {Varada, Mike, John, Hayley}. We want to take an SRS of size 2. The possible samples are:\n\nwithout replacement: {Varada, Mike}, {Varada, John}, {Mike, John}, {Varada, Hayley}, {Mike, Hayley}, {John, Hayley}\nwith replacement: {Varada, Mike}, {Varada, John}, {Mike, John}, {Varada, Hayley}, {Mike, Hayley}, {John, Hayley}, {Varada, Varada}, {Mike, Mike}, {John, John}, {Hayley, Hayley}\n\n\nYou might be asking yourself, why would we ever want to use SRS with replacement? If you first collected the information from Varada in Example 1.4, why would you allow the possibility of selecting Varada again? If we select the same element twice, we select repeated information and learn nothing new. So, in general, SRS without replacement is more informative, meaning that our parameter estimates will be more precise. But, unfortunately, there is a catch! SRS without replacement is not independent. The lack of independence violates our ?def-random-sample. However, as the population size increases compared to the sample size, SRS without replacement behaves very similarly to an independent sample (i.e., the joint distribution of the sample is very similar to the product of the marginal distributions). Remember the rule of thumb: n must be less than 10\\% the population size? This is the reason. But remember it is just a rule of thumb. You will explore this in more detail in ?exr-srs.\nOn the other hand, when we use SRS with replacement, we enforce the independence of the sample, no matter the population size. In addition, as the population size increases, the SRS with replacement becomes as informative as without replacement. Nonetheless, in practice we rarely use SRS with replacement.\nOne last comment about sampling. We will study what we call asymptotic properties of estimators. In other words, how the estimator behaves as the sample size becomes larger and larger (i.e. when the sample size goes to infinity). But what would that mean in the case of a finite population? If we keep increasing our sample size, we will eventually sample the entire population. In such a case, we would have a census, and no statistic is necessary because we would know the true value of the parameters. SRS with replacement allows us to take a sample larger than the population so that those results are still correct. But again, if you can afford (timewise and moneywise) to get a sample larger than the population, just do a census and be done with it. That being said, the asymptotic properties of estimators give us a lot of information about estimators’ behaviour, which is valuable to know.\n\n\nPopulation Distribution of finite populations\nThe population distribution is the distribution of the variable X over all the elements in the population. The problem with the population being finite is that even if the variable X is continuous (e.g., the student’s grade, people’s income, penguins’ height), the population distribution is discrete (i.e., its cdf is a step function; see the Figure 1.2). The problem is mitigated as the population becomes larger: the height of the step becomes shorter, and the spaces between the jumps are closer, resembling the cdf of a continuous variable as shown in Figure 1.3.\n\n\n\n\n\n\n\n\nFigure 1.3: Cumulative Distribution Function of 400 students’ grades in STAT 550 last year.\n\n\n\n\n\nAnother difficulty is that to estimate this population distribution from a sample, we need to obtain good estimates in every “small” region of the domain. This is a considerably harder task than estimating a few parameters, especially if there are multiple variables of interest.\nTo address these issues, we could use mathematical models to approximate the cdf of an actual finite population. For example, suppose that the Normal model approximates well the distribution of the population in Figure 1.3. We now have a continuous distribution for a continuous variable, and estimating the population is much simpler: we just need to find reasonable estimates for the mean and variance of the population, and we have a good approximation of the cdf of the population. As an extra, we also get all the theoretical developments about the normal distribution from probability. For example, we can tell that our population is symmetric, if we make any linear transformation on our variable, the population will still be Normal (e.g., if we want to give the grade out of 4, like a gpa, instead of out of 100).\n\n\n\n\n\n\n\n\nFigure 1.4: Cumulative Distribution Function of 400 students’ grades in STAT 550 last year and the Normal approximation (in red).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#take-away-points",
    "href": "intro.html#take-away-points",
    "title": "1  Introduction",
    "section": "1.4 Take-Away Points",
    "text": "1.4 Take-Away Points\n\nProbability distributions are “infinite” populations. We can sample an infinite population without replacement and still have independence, but this is not true for finite populations.\nRandom sample corresponds the n independent and identically distributed random variables. Do not mistake it for a Simple Random Sample, which is not independent in general.\nAny function of a random sample that does not depend on unknown parameters is a statistic.\nEstimating a parametric probability distribution from a sample is much easier, as we just need to estimate a few parameters instead of an entire curve (or step function).\nParametric space is the set of all possible values of the parameters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#exercises",
    "href": "intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Inference",
    "section": "",
    "text": "Welcome to Introduction to Statistical Inference for Data Science! In this course, we will cover\n\nIntroduction to statistical inference\nModule 1: sampling distribution of parameter estimates\n\nPopulation and samples\n\nModule 2: approximation of sampling distribution via bootstrapping\n\nSampling with and without replacement\n\nModule 3: the central limit theorem and mathematical approximation of the sampling distribution\n\nLaw of large numbers\nNormal distribution\nStandard deviation(?) and standard error\nCentral limit theorem\nSample distribution of mean, proportion, difference in mean? (talk about median? min?)\n\nModule 4: Confidence intervals via bootstrapping\nModule 5: Confidence intervals based on distributional assumptions\n\npdf, cdf, quantile, z-scores, t-distribution!!\n\nModule 6: Hypothesis Testing via randomization\n\nNull and alternative hypothesis\np-values\nType I and Type II errors\nOne sample proportion and mean\nTwo samples and permutation\n\nModule 7: Hypothesis testing based on distributional assumptions\n\nOne sample proportion - normal model\nOne sample mean - t-distribution\n\nModule 8: Errors in inference\n\nType I and Type II errors\nPower of a test\n\nModule 9: ANOVA and multiple hypothesis testing\nModule 10: A/B Testing\n\n\nPreface",
    "crumbs": [
      "Preface"
    ]
  }
]